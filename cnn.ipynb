{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "train_df = pd.read_csv(\"Data/train.csv\")\n",
    "test_df = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_df = train_df.drop(['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'],axis=1)\n",
    "test_df = test_df.drop(['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'],axis=1)\n",
    "\n",
    "# add image path to new column\n",
    "train_df[\"im_path\"] = \"Data/train/\" + train_df['Id'] + \".jpg\"\n",
    "test_df[\"im_path\"] = \"Data/train/\" + test_df['Id'] + \".jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I resize the images to 128 x 128 images for consistent, easier training. I augmented the dataset through grayscaling, blurring, and random flipping and perpestive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented data by transforming to grayscale, resizing, random flipping vertically and horizontally, handling the distortion of image, and transforming random perspective\n",
    "def augment_img(image, new_image_size):\n",
    "    transpose = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Grayscale(),\n",
    "                transforms.Resize(new_image_size),\n",
    "                transforms.RandomHorizontalFlip( p = 0.5),\n",
    "                transforms.RandomVerticalFlip(p = 0.5),\n",
    "                transforms.GaussianBlur((3,3)),\n",
    "                transforms.RandomPerspective(distortion_scale=0.5, p=0.5)])\n",
    "    image = transpose(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_size = (128, 128)\n",
    "  \n",
    "A, B = [], []\n",
    "for i in range(len(train_df)):\n",
    "    image = cv2.imread(train_df[\"im_path\"][i])\n",
    "    paw_score = train_df[\"Pawpularity\"][i]\n",
    "    image = augment_img(image, new_image_size)\n",
    "    A.append(image)\n",
    "    B.append(torch.from_numpy(np.array(paw_score)))\n",
    "\n",
    "A, B = torch.stack(A), torch.stack(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture:\n",
    "I use 6 layers for my CNN architecture:\n",
    "- Convolution Layers for extracting the features from the images.\n",
    "- Relu activation: increase the non-linearity in our images and since the image pixel is bigger than 0, keep pixels carrying a positive value.\n",
    "- Max Pooling layers: reduce parameters counts and computational complexity; prevent overfitting problems for the model.\n",
    "- Dropout Layers\n",
    "- Set of fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN architecture\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1,32,5,3), # size 42x42x32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64,3),  # size 40x40x64\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.Conv2d(64,128,3), # size 38x38x128\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2,2), # size 19x19x128\n",
    "            nn.Dropout2d(p = 0.1), # size 19x19x128\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128,256,3), # size 17x17x256\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Dropout2d(p = 0.1),\n",
    "            nn.Conv2d(256,512,3), # size 15x15x512\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512,1024,3,2), # size 7x7x1024\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.Flatten(), # size (50176)\n",
    "            nn.Linear(50176, 50), \n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(x.float(), y.float()))\n",
    "        return loss\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    RMSE = RMSELoss()\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = target.unsqueeze(1)\n",
    "        RMSE_loss = torch.sqrt(nn.MSELoss()(output.squeeze(0).float(), target.float()))\n",
    "        RMSE_loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx == 0: #Print loss for the first batch\n",
    "            print('Train Epoch: {}\\tRMSE_train: {:.6f}'.format(\n",
    "                epoch,  RMSE_loss.item()))\n",
    "                \n",
    "def validate(model, device, test_loader):\n",
    "    model.eval()\n",
    "    RMSE = RMSELoss()\n",
    "    loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss += RMSE.forward(output.squeeze(0),target)\n",
    "    print('RMSE_val: {:.6f}'.format((loss/len(test_loader.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "network = Network().to(device)\n",
    "optimizer = optim.Adam(network.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tRMSE_train: 36.597328\n",
      "Train Epoch: 1\tRMSE_train: 18.820772\n",
      "Train Epoch: 2\tRMSE_train: 19.676891\n",
      "Train Epoch: 3\tRMSE_train: 18.396284\n",
      "Train Epoch: 4\tRMSE_train: 24.029833\n",
      "Train Epoch: 5\tRMSE_train: 24.693430\n",
      "Train Epoch: 6\tRMSE_train: 20.745470\n",
      "Train Epoch: 7\tRMSE_train: 24.628674\n",
      "Train Epoch: 8\tRMSE_train: 26.161039\n",
      "Train Epoch: 9\tRMSE_train: 23.384872\n",
      "Train Epoch: 10\tRMSE_train: 26.452007\n",
      "Train Epoch: 11\tRMSE_train: 17.245005\n",
      "Train Epoch: 12\tRMSE_train: 20.148777\n",
      "Train Epoch: 13\tRMSE_train: 22.859543\n",
      "Train Epoch: 14\tRMSE_train: 19.329481\n",
      "Train Epoch: 15\tRMSE_train: 19.812038\n",
      "Train Epoch: 16\tRMSE_train: 20.769924\n",
      "Train Epoch: 17\tRMSE_train: 21.715115\n",
      "Train Epoch: 18\tRMSE_train: 22.511499\n",
      "Train Epoch: 19\tRMSE_train: 22.407368\n",
      "Train Epoch: 20\tRMSE_train: 18.071106\n",
      "Train Epoch: 21\tRMSE_train: 17.951204\n",
      "Train Epoch: 22\tRMSE_train: 20.234882\n",
      "Train Epoch: 23\tRMSE_train: 18.093975\n",
      "Train Epoch: 24\tRMSE_train: 13.387535\n",
      "Train Epoch: 25\tRMSE_train: 16.562382\n",
      "Train Epoch: 26\tRMSE_train: 14.356655\n",
      "Train Epoch: 27\tRMSE_train: 14.780622\n",
      "Train Epoch: 28\tRMSE_train: 18.630884\n",
      "Train Epoch: 29\tRMSE_train: 14.496690\n",
      "Train Epoch: 30\tRMSE_train: 12.731870\n",
      "Train Epoch: 31\tRMSE_train: 14.536271\n",
      "Train Epoch: 32\tRMSE_train: 14.250739\n",
      "Train Epoch: 33\tRMSE_train: 13.881637\n",
      "Train Epoch: 34\tRMSE_train: 13.109826\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 35\n",
    "dataset = utils.TensorDataset(A, B)\n",
    "train_dataset,test_dataset = utils.random_split(dataset, lengths  =[round(len(A) * 0.8),len(A) - round(len(A) * 0.8)], generator=torch.Generator().manual_seed(16))\n",
    "train_loader = utils.DataLoader(train_dataset,shuffle = True, batch_size = batch_size)\n",
    "test_loader = utils.DataLoader(test_dataset,shuffle = False)\n",
    "for epoch in range(epochs):\n",
    "        train(network, device, train_loader, optimizer, epoch)\n",
    "torch.save(network.state_dict(), \"PET.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE_val: 17.619154\n"
     ]
    }
   ],
   "source": [
    "validate(network,device,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, I received a RMSE score of 17.619, which is in the top 10 of the Kaggle Leaderboard. My solution could be adapted into AI tools that will guide shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and recommending composition improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = pd.DataFrame()\n",
    "cnn['Id'] = test_df['Id']\n",
    "test = []\n",
    "for index in range(len(test_df[\"im_path\"])):\n",
    "    image = cv2.imread(train_df[\"im_path\"][i])\n",
    "    image = augment_img(image, new_image_size)\n",
    "    image = image.to(device)\n",
    "    score = network(image[None,:,:,:]).cpu().detach().numpy()\n",
    "    test. append(score[0,0])\n",
    "cnn['Pawpularity'] = test\n",
    "cnn.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10caff7fae0b833e5f3c9b220ab9e808885818ca61ac39d08600351c08ac2905"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('miniproject': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
